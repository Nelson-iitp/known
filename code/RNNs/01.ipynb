{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [ global ]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sys.version='3.10.2 (tags/v3.10.2:a58ebcc, Jan 17 2022, 14:12:15) [MSC v.1929 64 bit (AMD64)]'\n",
      "np.__version__='1.24.1'\n",
      "tt.__version__='1.13.1+cpu'\n",
      "known.__version__='0.0.1'\n"
     ]
    }
   ],
   "source": [
    "# inbuilt \n",
    "import os\n",
    "import sys\n",
    "import math\n",
    "\n",
    "# most common\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# pytorch\n",
    "import torch as tt\n",
    "import torch.nn as nn\n",
    "import torch.optim as oo\n",
    "import torch.functional as ff\n",
    "import torch.distributions as dd\n",
    "import torch.utils.data as ud\n",
    "\n",
    "# custom\n",
    "import known\n",
    "import known.ktorch as kt\n",
    "from known.basic import pj\n",
    "print(f'{sys.version=}\\n{np.__version__=}\\n{tt.__version__=}\\n{known.__version__=}')\n",
    "\n",
    "from torch.utils.data import Dataset, IterableDataset, DataLoader\n",
    "import glob\n",
    "\n",
    "import unicodedata\n",
    "import string\n",
    "\n",
    "import known \n",
    "import known.ktorch as kt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NamesData(DataLoader):\n",
    "    def findFiles(path): return glob.glob(path)\n",
    "\n",
    "    \n",
    "    all_letters = string.ascii_letters + \" .,;'\"\n",
    "    n_letters = len(all_letters)\n",
    "\n",
    "\n",
    "    def unicodeToAscii(s):\n",
    "        return ''.join(\n",
    "            c for c in unicodedata.normalize('NFD', s)\n",
    "            if unicodedata.category(c) != 'Mn'\n",
    "            and c in __class__.all_letters\n",
    "        )\n",
    "    \n",
    "\n",
    "    # Read a file and split into lines\n",
    "    def readLines(filename):\n",
    "        lines = open(filename, encoding='utf-8').read().strip().split('\\n')\n",
    "        return [__class__.unicodeToAscii(line) for line in lines]\n",
    "\n",
    "    def letterToIndex(letter): return __class__.all_letters.find(letter)\n",
    "\n",
    "    def letterToTensor(letter):\n",
    "        tensor = tt.zeros(1, __class__.n_letters)\n",
    "        tensor[0][__class__.letterToIndex(letter)] = 1\n",
    "        return tensor\n",
    "\n",
    "    def lineToTensor(line):\n",
    "        tensor = tt.zeros(len(line), 1, __class__.n_letters)\n",
    "        for li, letter in enumerate(line):\n",
    "            tensor[li][0][__class__.letterToIndex(letter)] = 1\n",
    "        return tensor\n",
    "\n",
    "    def __len__(self):\n",
    "        return  self.length\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        category = self.all_categories[self.rng.integers(0, len(self.all_categories))]\n",
    "        lines = self.category_lines[category]\n",
    "        line =lines[self.rng.integers(0, len(lines))]\n",
    "        category_tensor = tt.zeros(self.n_categories, dtype=tt.long)\n",
    "        category_tensor[self.all_categories.index(category)]=1\n",
    "        line_tensor = __class__.lineToTensor(line)\n",
    "        self.last=(line, category)\n",
    "        return line_tensor.squeeze_(1), category_tensor#.squeeze_(0)\n",
    "\n",
    "    def __init__(self, path = 'data/names/*.txt', seed=None):\n",
    "        fileslike = __class__.findFiles(path)\n",
    "        self.category_lines = {}\n",
    "        self.all_categories = []\n",
    "        for filename in fileslike:\n",
    "            category = os.path.splitext(os.path.basename(filename))[0]\n",
    "            self.all_categories.append(category)\n",
    "            lines =__class__. readLines(filename)\n",
    "            self.category_lines[category] = lines\n",
    "        self.n_categories = len(self.all_categories)\n",
    "        self.length = sum([len(v) for v in self.category_lines.values()])\n",
    "        self.rng = np.random.default_rng(seed)\n",
    "\n",
    "    def categoryFromOutput(self, output):\n",
    "        top_n, top_i = output.topk(1)\n",
    "        print(top_n, top_i)\n",
    "        category_i = top_i[0].item()\n",
    "        return self.all_categories[category_i], category_i"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = NamesData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "dl=iter(DataLoader(ds, batch_size=1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "type: <class 'torch.Tensor'>\n",
      "len: 1\n",
      "shape: torch.Size([1, 7, 57])\n",
      "type: <class 'torch.Tensor'>\n",
      "len: 1\n",
      "shape: torch.Size([1, 18])\n",
      "Object:\n",
      "tensor([[0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "('Bernard', 'French')\n"
     ]
    }
   ],
   "source": [
    "x,y = next(dl)\n",
    "\n",
    "\n",
    "known.about(x)\n",
    "known.about(y, True)\n",
    "print(ds.last)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnnc = kt.ELMANC(\n",
    "    has_bias=True,\n",
    "    actF=lambda x: tt.softmax(x, dim=1),\n",
    "    input_size=NamesData.n_letters,         # input features\n",
    "    hidden_sizes=[128, ds.n_categories],       # hidden features at each layer\n",
    "    dropout=0.0,        # dropout after each layer, only if hidden_sizes > 1\n",
    "    batch_first=True,  # if true, excepts input as (batch_size, seq_len, input_size) else (seq_len, batch_size, input_size)\n",
    "    dtype=tt.float32,\n",
    "    device=None,\n",
    "    stack_output=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with tt.no_grad():\n",
    "    P, H = rnnc(x)\n",
    "known.about(P)\n",
    "known.about(H)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model=rnnc\n",
    "epochs = 10_0000\n",
    "batch_size=1\n",
    "shuffle=True\n",
    "validation_freq = int(epochs/10)\n",
    "criterion=nn.NLLLoss()\n",
    "lr = 0.005\n",
    "weight_decay = 0.0\n",
    "optimizer=oo.Adam(rnnc.parameters(), lr=lr, weight_decay=weight_decay)\n",
    "lrs=oo.lr_scheduler.LinearLR(optimizer, start_factor= 1.0, end_factor=0.7, total_iters=epochs)\n",
    "\n",
    "early_stop_train=kt.QuantiyMonitor('TrainLoss', patience=50, delta=0.00001)\n",
    "early_stop_val=kt.QuantiyMonitor('ValLoss', patience=50, delta=0.00001)\n",
    "checkpoint_freq=int(epochs/4)\n",
    "save_path='sample.rnn'\n",
    "loss_plot_start = int(epochs/50)\n",
    "\n",
    "trainer = kt.Trainer(model)\n",
    "trainer.optimizer=optimizer\n",
    "trainer.criterion=criterion\n",
    "\n",
    "trainer.fit(training_data=ds, validation_data=None, \n",
    "            epochs=epochs, batch_size=batch_size,shuffle=shuffle,validation_freq=validation_freq,\n",
    "            save_path=save_path, use_rnn=True, verbose=1)\n",
    "\n",
    "trainer.plot_results(loss_plot_start=loss_plot_start)\n",
    "\n",
    "mtl, tl = trainer.evaluate(ds, use_rnn=True)\n",
    "print('loss', mtl)\n",
    "print('=================================================')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(input_line, n_predictions=3):\n",
    "    xaxis = ds.all_categories\n",
    "    print('\\n> %s' % input_line)\n",
    "    with tt.no_grad():\n",
    "        output, *_ = rnnc(NamesData.lineToTensor(input_line))\n",
    "        print(output.shape)\n",
    "    for i,ts in enumerate(output):\n",
    "        plt.figure(figsize=(20,3))\n",
    "        plt.title(f'{i+1}')\n",
    "        plt.bar(xaxis, ts[0])\n",
    "        plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('Dovesky')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('Jackson')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict('Satoshi')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "authorship_tag": "ABX9TyOuUs+vkPNCQj5Gz+GQVKU9",
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "014_siameseNetwork.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "__venv310__",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.2"
  },
  "vscode": {
   "interpreter": {
    "hash": "8fe6015140738806d41bf643a20b25ab509e1558f046cb4d7778c2b8bb5aa7b5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
